mode: deployment

image:
  repository: otel/opentelemetry-collector-contrib
  tag: "0.141.0"  # Latest stable as of Dec 2025

replicaCount: 3  # Prod: 3 replicas for HA

resources:
  requests:
    cpu: "500m"
    memory: "2Gi"
  limits:
    cpu: "2"
    memory: "4Gi"

service:
  type: ClusterIP

podDisruptionBudget:
  enabled: true
  minAvailable: 2

autoscaling:
  enabled: true
  minReplicas: 3
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70

config:
  extensions:
    health_check:
      endpoint: "0.0.0.0:13133"
    pprof:
      endpoint: "0.0.0.0:1777"
    zpages:
      endpoint: "0.0.0.0:55679"

  receivers:
    otlp:
      protocols:
        grpc:
          endpoint: "0.0.0.0:4317"
          max_recv_msg_size_mib: 32
        http:
          endpoint: "0.0.0.0:4318"
    jaeger:
      protocols:
        grpc:
          endpoint: "0.0.0.0:14250"
        thrift_http:
          endpoint: "0.0.0.0:14268"
    prometheus:
      config:
        scrape_configs:
          - job_name: "otel-collector"
            scrape_interval: 30s
            static_configs:
              - targets: ["0.0.0.0:8888"]

  processors:
    batch:
      timeout: 10s
      send_batch_size: 1024
      send_batch_max_size: 2048
    memory_limiter:
      check_interval: 1s
      limit_mib: 3072
      spike_limit_mib: 512
    resource:
      attributes:
        - key: deployment.environment
          value: "production"
          action: upsert
        - key: cluster.name
          value: "restaurant-prod"
          action: upsert
    attributes:
      actions:
        - key: http.request.header.authorization
          action: delete
        - key: http.request.header.cookie
          action: delete
    # Production tail sampling - keep errors + slow + 10% random
    tail_sampling:
      decision_wait: 10s
      num_traces: 50000
      expected_new_traces_per_sec: 100
      policies:
        - name: errors
          type: status_code
          status_code:
            status_codes: [ERROR]
        - name: slow-requests
          type: latency
          latency:
            threshold_ms: 1000
        - name: random-sample
          type: probabilistic
          probabilistic:
            sampling_percentage: 10

  exporters:
    otlp/jaeger:
      endpoint: "jaeger-collector.observability.svc.cluster.local:4317"
      tls:
        insecure: true
      retry_on_failure:
        enabled: true
    otlphttp/logstash:
      endpoint: "http://logstash-logstash.observability.svc.cluster.local:8080"
      tls:
        insecure: true
      retry_on_failure:
        enabled: true
    prometheus:
      endpoint: "0.0.0.0:8888"
      namespace: "otel"
      const_labels:
        environment: "production"
    debug:
      verbosity: normal  # Prod: normal (detailed only for troubleshooting)

  service:
    extensions: [health_check, pprof, zpages]
    telemetry:
      logs:
        level: "info"
      metrics:
        address: "0.0.0.0:8888"
    pipelines:
      traces:
        receivers: [otlp, jaeger]
        processors: [memory_limiter, resource, attributes, tail_sampling, batch]
        exporters: [otlp/jaeger, debug]
      metrics:
        receivers: [otlp, prometheus]
        processors: [memory_limiter, resource, batch]
        exporters: [prometheus, debug]
      logs:
        receivers: [otlp]
        processors: [memory_limiter, resource, attributes, batch]
        exporters: [otlphttp/logstash, debug]